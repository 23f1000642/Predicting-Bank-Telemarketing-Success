{ 
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 85062,
          "databundleVersionId": 9578279,
          "sourceType": "competition"
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "name": "23f1000642-notebook-t32024.",
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/23f1000642/Predicting-Bank-Telemarketing-Success/blob/main/23f1000642_notebook_t32024_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "Uqb4AVfGaTss"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "predict_the_success_of_bank_telemarketing_path = kagglehub.competition_download('predict-the-success-of-bank-telemarketing')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "1LAtcnK6aTsw"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Title: Predicting Bank Telemarketing Success Using Machine Learning"
      ],
      "metadata": {
        "id": "i0DddBSyaTsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### :::::::::: :::::::::: :::::::::: :::::::::: ::::::::::  Process of The Project  :::::::::: :::::::::: :::::::::: :::::::::: ::::::::::\n",
        "\n",
        "                                - 1 . python Implementation\n",
        "                                - 2 . Domain Analysis ,  Basic Checks & find Insights\n",
        "                                - 3 . EDA (univariate , Bi-Variate ) and Find Insights\n",
        "                                - 4 . Data Preprocessing / Feature engineering\n",
        "                \n",
        "                                                        - ( i ) . Find Missing values & impute them\n",
        "                                                        - ( ii ) .  Convert Categorical variable to Numerical\n",
        "                                                       \n",
        "                                        \n",
        "                                - 5 . Feature Selection\n",
        "                                                         - ( i ) . Check coorelation\n",
        "                                                         - ( ii ) . use pipeline\n",
        "                                                         - ( iii ) . Create Dependent(y) & Independent(X) Variable\n",
        "                                                    \n",
        "                                - 6 . Model Creation  & Evaluation\n",
        "                                        \n",
        "                                                          - ( i ) . Splite Data into Train & Test\n",
        "                                                          - ( ii ) . Import Matrics for evaluation\n",
        "                                                          \n",
        "                                                                        - ( a ) . Logistic regression\n",
        "                                                                        - ( b ) . K Nearest Neighbour\n",
        "                                                                        - ( c ) . Lightgbm\n",
        "                                                                        - ( d ) . Random Forest\n",
        "                                                                        - ( e ) . XG - Boost\n",
        "                                                                        \n",
        "                                - 7 . Create Model Comparision Report\n",
        "### ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::"
      ],
      "metadata": {
        "id": "R69UJpnNaTs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Objective:***\n",
        "### The goal of the project is to predict whether a customer will subscribe to a bank term deposit (yes or no) based on features such as contact details, demographics, and campaign-related data from previous telemarketing calls."
      ],
      "metadata": {
        "id": "gDMckXQ3aTs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 . Python Implementation"
      ],
      "metadata": {
        "id": "f48f63ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries\n"
      ],
      "metadata": {
        "id": "8QeSQt46aTs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import  f1_score, precision_score, recall_score, precision_recall_curve\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import uniform, randint\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "\n",
        "\n",
        "# import seaborn library for data visualisation\n",
        "%matplotlib inline\n",
        "\n",
        "# Import Warnings For ignoring Feature Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "12f07bf1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:51.109775Z",
          "iopub.execute_input": "2024-11-26T12:10:51.110969Z",
          "iopub.status.idle": "2024-11-26T12:10:55.783097Z",
          "shell.execute_reply.started": "2024-11-26T12:10:51.110923Z",
          "shell.execute_reply": "2024-11-26T12:10:55.781937Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "data = pd.read_csv('/kaggle/input/predict-the-success-of-bank-telemarketing/train.csv')\n",
        "\n",
        "test = pd.read_csv('/kaggle/input/predict-the-success-of-bank-telemarketing/test.csv')\n"
      ],
      "metadata": {
        "id": "ac5d6029",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:55.785452Z",
          "iopub.execute_input": "2024-11-26T12:10:55.786114Z",
          "iopub.status.idle": "2024-11-26T12:10:55.955192Z",
          "shell.execute_reply.started": "2024-11-26T12:10:55.786066Z",
          "shell.execute_reply": "2024-11-26T12:10:55.954064Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 . Domain Analysis and Basic Checks"
      ],
      "metadata": {
        "id": "ab6bd85b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to see first five records\n",
        "data.head()"
      ],
      "metadata": {
        "id": "cab77e0d",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:55.956629Z",
          "iopub.execute_input": "2024-11-26T12:10:55.956949Z",
          "iopub.status.idle": "2024-11-26T12:10:55.98283Z",
          "shell.execute_reply.started": "2024-11-26T12:10:55.95692Z",
          "shell.execute_reply": "2024-11-26T12:10:55.981813Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# to see all columns\n",
        "pd.set_option('display.max_columns',None)\n",
        "data.tail() # to see last 5 records\n"
      ],
      "metadata": {
        "id": "f9e8ddc5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:55.985041Z",
          "iopub.execute_input": "2024-11-26T12:10:55.985396Z",
          "iopub.status.idle": "2024-11-26T12:10:56.00039Z",
          "shell.execute_reply.started": "2024-11-26T12:10:55.985336Z",
          "shell.execute_reply": "2024-11-26T12:10:55.999234Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# to know about data type & null values\n",
        "data.info()"
      ],
      "metadata": {
        "id": "5c95db5e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:56.001643Z",
          "iopub.execute_input": "2024-11-26T12:10:56.002053Z",
          "iopub.status.idle": "2024-11-26T12:10:56.053885Z",
          "shell.execute_reply.started": "2024-11-26T12:10:56.002021Z",
          "shell.execute_reply": "2024-11-26T12:10:56.052873Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Variables:\n",
        "1. **last contact date**: Last contact date.  \n",
        "2. **age**: Age of the client (numeric).  \n",
        "3. **job**: Type of job.  \n",
        "4. **marital**: Marital status (categorical: `\"married\"`, `\"divorced\"`, `\"single\"`; note: `\"divorced\"` includes divorced or widowed).  \n",
        "5. **education**: Level of education (categorical: `\"unknown\"`, `\"secondary\"`, `\"primary\"`, `\"tertiary\"`).  \n",
        "6. **default**: Does the client have credit in default? (binary: `\"yes\"`, `\"no\"`).  \n",
        "7. **balance**: Average yearly balance in euros (numeric).  \n",
        "8. **housing**: Does the client have a housing loan? (binary: `\"yes\"`, `\"no\"`).  \n",
        "9. **loan**: Does the client have a personal loan? (binary: `\"yes\"`, `\"no\"`).  \n",
        "10. **contact**: Contact communication type (categorical: `\"unknown\"`, `\"telephone\"`, `\"cellular\"`).  \n",
        "11. **duration**: Last contact duration in seconds (numeric).  \n",
        "12. **campaign**: Number of contacts performed during this campaign and for this client (numeric, includes the last contact).  \n",
        "13. **pdays**: Number of days since the client was last contacted in a previous campaign (numeric, `-1` means the client was not previously contacted).  \n",
        "14. **previous**: Number of contacts performed before this campaign for this client (numeric).  \n",
        "15. **poutcome**: Outcome of the previous marketing campaign (categorical: `\"unknown\"`, `\"other\"`, `\"failure\"`, `\"success\"`).  \n",
        "\n",
        "### Output Variable (Desired Target):\n",
        "16. **target**: Has the client subscribed to a term deposit? (binary: `\"yes\"`, `\"no\"`).\n"
      ],
      "metadata": {
        "id": "a3Y0AI72aTs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To get Statistical information about numerical columns\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "83f96c82",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:56.055124Z",
          "iopub.execute_input": "2024-11-26T12:10:56.055461Z",
          "iopub.status.idle": "2024-11-26T12:10:56.093757Z",
          "shell.execute_reply.started": "2024-11-26T12:10:56.055429Z",
          "shell.execute_reply": "2024-11-26T12:10:56.09272Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights for Numerical  Datatype : --\n",
        "* **Age**: Average age is 42 years, ranging from 18 to 95 years.\n",
        "* **Balance**: Mean account balance is €5441, with high variability (up to €102,127).\n",
        "* **Call Duration**: Average call duration is 439 seconds, with a maximum of 4918 seconds.\n",
        "* **Campaign Contacts**: Median contacts per person is 2, but some were contacted up to 63 times.\n",
        "* **Previous Campaigns**: Most clients were not previously contacted (pdays = -1 for many).\n",
        "* **Target Variable**: Only 14.86% of clients subscribed, showing class imbalance.\n",
        "* **Temporal Data**: Most campaign activity occurred in 2009, with June being the busiest month.\n",
        "\n"
      ],
      "metadata": {
        "id": "745147bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To know statistical information about Categorical data\n",
        "data.describe(include='O')"
      ],
      "metadata": {
        "id": "c43fb159",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:56.094994Z",
          "iopub.execute_input": "2024-11-26T12:10:56.095275Z",
          "iopub.status.idle": "2024-11-26T12:10:56.163991Z",
          "shell.execute_reply.started": "2024-11-26T12:10:56.095248Z",
          "shell.execute_reply": "2024-11-26T12:10:56.162909Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Count**: Non-null values in each column; `job`, `education`, `contact` have missing data.\n",
        "- **Unique**: Indicates diversity in values; e.g., `job` has 11 roles, `poutcome` has 3 outcomes.\n",
        "- **Top**: Most frequent value per column (e.g., \"married\" in `marital`).\n",
        "- **Frequency**: Frequency of top values, showing class imbalance like \"no\" in `target`."
      ],
      "metadata": {
        "id": "8SusL56AaTs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 . EDA : Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "52e03395"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:56.165122Z",
          "iopub.execute_input": "2024-11-26T12:10:56.165453Z",
          "iopub.status.idle": "2024-11-26T12:10:56.194445Z",
          "shell.execute_reply.started": "2024-11-26T12:10:56.165424Z",
          "shell.execute_reply": "2024-11-26T12:10:56.193118Z"
        },
        "id": "VNIFhJ22aTs9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data['last contact date'].head()\n"
      ],
      "metadata": {
        "id": "fH6OLDX3eVMH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:56.196018Z",
          "iopub.execute_input": "2024-11-26T12:10:56.196324Z",
          "iopub.status.idle": "2024-11-26T12:10:56.20445Z",
          "shell.execute_reply.started": "2024-11-26T12:10:56.196296Z",
          "shell.execute_reply": "2024-11-26T12:10:56.203415Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data['last contact date'] = pd.to_datetime(data['last contact date'])\n",
        "data['p_year'] = data['last contact date'].dt.year\n",
        "data['p_month'] = data['last contact date'].dt.month\n",
        "data['p_weekday'] = data['last contact date'].dt.weekday\n",
        "\n",
        "\n",
        "test['last contact date'] = pd.to_datetime(test['last contact date'])\n",
        "test['p_year'] = test['last contact date'].dt.year\n",
        "test['p_month'] = test['last contact date'].dt.month\n",
        "test['p_weekday'] = test['last contact date'].dt.weekday"
      ],
      "metadata": {
        "id": "WypIzdVBegas",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:56.20828Z",
          "iopub.execute_input": "2024-11-26T12:10:56.209215Z",
          "iopub.status.idle": "2024-11-26T12:10:56.241007Z",
          "shell.execute_reply.started": "2024-11-26T12:10:56.209168Z",
          "shell.execute_reply": "2024-11-26T12:10:56.239916Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Yearly Trends*: Spot long-term shifts in customer behavior or campaign effectiveness by analyzing engagement patterns over years.\n",
        "\n",
        "*Monthly Patterns*: Reveal seasonal effects, potentially indicating which months drive higher engagement.\n",
        "\n",
        "*Weekday Variations*: Understand customer preferences for specific days, aiding in scheduling more impactful marketing outreach.\n",
        "\n",
        "These extracted features enrich the data, allowing the model to leverage temporal patterns for better prediction accuracy."
      ],
      "metadata": {
        "id": "fj2b0C26aTs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head()),\n",
        "print('-----------------------------------------------------------------------'),\n",
        "print(test.head())"
      ],
      "metadata": {
        "id": "6L6OHZSGejzU",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:56.242164Z",
          "iopub.execute_input": "2024-11-26T12:10:56.242499Z",
          "iopub.status.idle": "2024-11-26T12:10:56.257289Z",
          "shell.execute_reply.started": "2024-11-26T12:10:56.24246Z",
          "shell.execute_reply": "2024-11-26T12:10:56.256035Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(['last contact date'],axis=1)\n",
        "test = test.drop(['last contact date'],axis=1)"
      ],
      "metadata": {
        "id": "iCpTXeQYeqXx",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:56.258436Z",
          "iopub.execute_input": "2024-11-26T12:10:56.258783Z",
          "iopub.status.idle": "2024-11-26T12:10:56.279391Z",
          "shell.execute_reply.started": "2024-11-26T12:10:56.258753Z",
          "shell.execute_reply": "2024-11-26T12:10:56.27855Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping the `last contact date` column was necessary after extracting `year`, `month`, and `weekday` features. This removed redundant information, streamlined the dataset, and reduced potential noise, focusing the model on the more relevant, time-based features for improved accuracy."
      ],
      "metadata": {
        "id": "zympolh_aTs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using a for loop to find NoN Objective data type\n",
        "data1 = []                         # create a empty List\n",
        "for i in data.columns:             # use for loop in data.columns\n",
        "    if data[i].dtype!='O':         # use if condition\n",
        "        data1.append(i)            # Append that columns which satisfy the if condition\n",
        "print(data1)                       # print the List"
      ],
      "metadata": {
        "id": "9ef03e30",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:56.280611Z",
          "iopub.execute_input": "2024-11-26T12:10:56.281006Z",
          "iopub.status.idle": "2024-11-26T12:10:56.287807Z",
          "shell.execute_reply.started": "2024-11-26T12:10:56.280966Z",
          "shell.execute_reply": "2024-11-26T12:10:56.286723Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = data[data1]                  # create a Numerical data type Variable\n",
        "data2 = data.drop(data1,axis=1)      # create a categorical data type variable"
      ],
      "metadata": {
        "id": "0d806750",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:56.289101Z",
          "iopub.execute_input": "2024-11-26T12:10:56.289501Z",
          "iopub.status.idle": "2024-11-26T12:10:56.30789Z",
          "shell.execute_reply.started": "2024-11-26T12:10:56.289461Z",
          "shell.execute_reply": "2024-11-26T12:10:56.306704Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ( i ) . Univariate Analysis"
      ],
      "metadata": {
        "id": "0adefd47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## univariate analysis for Numerical Columns\n",
        "plt.figure(figsize=(10,10))                                         # set the canvas size\n",
        "plotnumber = 1                                                      # create a variable for plotting numbers\n",
        "for i in data1 :                                                    # use for loop to iterate the loop\n",
        "    plt.subplot(5,2,plotnumber)                                     # use subplot to plot figure in rows & columns\n",
        "    sns.histplot(x = data1[i],color='blue',kde=True)                # plot Histogram\n",
        "    plotnumber = plotnumber + 1\n",
        "plt.tight_layout()                                                  # to avoid the overlapping"
      ],
      "metadata": {
        "id": "a092d14c",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:10:56.309416Z",
          "iopub.execute_input": "2024-11-26T12:10:56.309848Z",
          "iopub.status.idle": "2024-11-26T12:11:02.755861Z",
          "shell.execute_reply.started": "2024-11-26T12:10:56.309803Z",
          "shell.execute_reply": "2024-11-26T12:11:02.754589Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Purpose**: Conduct univariate analysis on numerical columns to understand data distribution.\n",
        "- **Result**: Visual distribution of each numeric column, highlighting patterns and skewness."
      ],
      "metadata": {
        "id": "JRAGj4YSaTtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights of Numerical data  :--\n",
        "\n",
        "1. **Age**: The age distribution is right-skewed, with a majority of clients around 30–40 years. Older age groups have fewer clients.\n",
        "\n",
        "2. **Balance**: The balance feature is highly skewed, with most clients having a low balance. A few have exceptionally high balances.\n",
        "\n",
        "3. **Duration**: Duration shows a right-skewed distribution, indicating that most calls were relatively short, while a few were significantly longer.\n",
        "\n",
        "4. **Campaign**: Most clients were contacted only a few times in a single campaign, as indicated by the right-skewed distribution.\n",
        "\n",
        "5. **Pdays**: Most values are clustered around zero, suggesting that most clients had not been contacted in the previous campaign.\n",
        "\n",
        "6. **Previous**: Similar to `pdays`, most clients had no prior contacts, with very few having multiple contacts.\n",
        "\n",
        "7. **P_year**: Peaks around specific years suggest that certain years had higher client activity or data collection intensity.\n",
        "\n",
        "8. **P_month**: Client contacts show seasonal peaks, with higher counts in specific months, indicating targeted campaigns during certain times of the year.\n",
        "\n",
        "9. **P_weekday**: There are consistent client contact patterns across weekdays, showing no significant preference foature engineering.\n",
        "\n"
      ],
      "metadata": {
        "id": "b911c225"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## univariate analysis for Categorical Columns\n",
        "plt.figure(figsize=(10,18))                                         # set the  size\n",
        "plotnumber = 1                                                      # create a variable for plotting numbers\n",
        "for i in data2 :                                                    # use for loop to iterate the loop\n",
        "    plt.subplot(6,2,plotnumber)                                     # use subplot to plot figure in rows & columns\n",
        "    sns.countplot(x = data2[i])                                     # plot countplot\n",
        "    plotnumber = plotnumber + 1\n",
        "\n",
        "    plt.xticks(rotation=90)\n",
        "plt.tight_layout()                                                  # to avoid the overlapping"
      ],
      "metadata": {
        "id": "bae54760",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:02.757542Z",
          "iopub.execute_input": "2024-11-26T12:11:02.758095Z",
          "iopub.status.idle": "2024-11-26T12:11:04.220437Z",
          "shell.execute_reply.started": "2024-11-26T12:11:02.758034Z",
          "shell.execute_reply": "2024-11-26T12:11:04.219188Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights Categorical Data :--\n",
        "\n",
        "\n",
        "**Specific Insights:**\n",
        "\n",
        "* **Job:** The most common job types are \"blue-collar\" and \"management.\"\n",
        "* **Marital:** The majority of individuals are \"married,\" followed by \"single\" and \"divorced.\"\n",
        "* **Education:** The most common education level is \"secondary,\" followed by \"tertiary\" and \"primary.\"\n",
        "* **Default:** Most individuals have not defaulted on their credit.\n",
        "* **Housing:** The majority of individuals have a housing loan.\n",
        "* **Loan:** A significant number of individuals have a personal loan.\n",
        "* **Contact:** The most common mode of contact is \"cellular,\" followed by \"telephone.\"\n",
        "* **Poutcome:** The most common outcome of the previous marketing campaign is \"failure.\"\n",
        "\n",
        "  \n",
        "\n",
        "**Overall:**\n",
        "\n",
        "* The dataset appears to be imbalanced, with a higher proportion of individuals who did not subscribe to a term deposit (\"no\").\n",
        "\n"
      ],
      "metadata": {
        "id": "SFpLpd38aTtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ( ii ) . Bivariate Analysis"
      ],
      "metadata": {
        "id": "02b30368"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Bivariate analysis for Numerical Columns\n",
        "plt.figure(figsize=(10,18))                                         # set the canvas size\n",
        "plotnumber = 1                                                      # create a variable for plotting numbers\n",
        "for i in data1 :                                                    # use for loop to iterate the loop\n",
        "    plt.subplot(6,2,plotnumber)                                     # use subplot to plot figure in rows & columns\n",
        "    sns.histplot(x = data1[i],hue=data.target,palette=['red', 'green'])       # plot Histogram with Target\n",
        "    plotnumber = plotnumber + 1\n",
        "    plt.xticks(rotation=90)\n",
        "plt.tight_layout()                                                  # to avoid the overlapping"
      ],
      "metadata": {
        "id": "5140b585",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:04.221991Z",
          "iopub.execute_input": "2024-11-26T12:11:04.222442Z",
          "iopub.status.idle": "2024-11-26T12:11:14.113685Z",
          "shell.execute_reply.started": "2024-11-26T12:11:04.222397Z",
          "shell.execute_reply": "2024-11-26T12:11:14.112421Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights of Numerical data with Targetts:**\n",
        "\n",
        "\n",
        "1. **`age`:** Subscription rates are higher for individuals in their 30s and decline with age.  \n",
        "2. **`balance`:** Positive balances slightly increase subscription rates; most balances are near zero.  \n",
        "3. **`duration`:** Longer contact durations strongly correlate with higher subscriptions.  \n",
        "4. **`campaign`:** Multiple calls (above 10) do not significantly improve subscription rates.  \n",
        "5. **`pdays`:** Non-zero `pdays` (prior contact) positively influence subscriptions.  \n",
        "6. **`previous`:** Higher prior successful contacts slightly increase subscription likelihood.  \n",
        "7. **`p_year`:** Campaigns in 2009 had better subscription rates compared to 2008 and 2010.  \n",
        "8. **`p_month`:** Subscriptions are higher in March, September, and December, showing seasonality.  \n",
        "9. **`p_weekday`:** Subscription rates are consistent across weekdays. conclusions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e248fe42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Bivariate analysis for Categorical Columns\n",
        "plt.figure(figsize=(10,18))                                         # set the canvas size\n",
        "plotnumber = 1                                                      # create a variable for plotting numbers\n",
        "for i in data2.drop('target',axis=1) :                                   # use for loop to iterate the loop\n",
        "    plt.subplot(6,2,plotnumber)                                     # use subplot to plot figure in rows & columns\n",
        "    sns.countplot(x = data2[i],hue=data.target,palette=['r', 'g'])       # plot Countplot with Target class\n",
        "    plotnumber = plotnumber + 1\n",
        "    plt.xticks(rotation=90)\n",
        "plt.tight_layout()                                                  # to avoid the overlapping"
      ],
      "metadata": {
        "id": "83f81334",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:14.11529Z",
          "iopub.execute_input": "2024-11-26T12:11:14.116082Z",
          "iopub.status.idle": "2024-11-26T12:11:16.149074Z",
          "shell.execute_reply.started": "2024-11-26T12:11:14.116033Z",
          "shell.execute_reply": "2024-11-26T12:11:16.14772Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights of Categorical data with Targets:**\n",
        "\n",
        "* **Job:** The most common job types are \"blue-collar\" and \"management.\"\n",
        "* **Marital:** The majority of individuals are \"married,\" followed by \"single\" and \"divorced.\"\n",
        "* **Education:** The most common education level is \"secondary,\" followed by \"tertiary\" and \"primary.\"\n",
        "* **Default:** Most individuals have not defaulted on their credit.\n",
        "* **Housing:** The majority of individuals have a housing loan.\n",
        "* **Loan:** A significant number of individuals have a personal loan.\n",
        "* **Contact:** The most common mode of contact is \"cellular,\" followed by \"telephone.\"\n",
        "* **Poutcome:** The most common outcome of the previous marketing campaign is \"fat conclusions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "675c10a5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 . Data Preprocessing :"
      ],
      "metadata": {
        "id": "21bae4a5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ( i ) . Handle missing values :-"
      ],
      "metadata": {
        "id": "0c2fbbe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see number of missing values present in each Features\n",
        "print(data.isnull().sum())\n",
        "print(\"--------------------------------------------------------------\")\n",
        "\n",
        "print(test.isnull().sum())"
      ],
      "metadata": {
        "id": "46066ba4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:16.150953Z",
          "iopub.execute_input": "2024-11-26T12:11:16.151393Z",
          "iopub.status.idle": "2024-11-26T12:11:16.182048Z",
          "shell.execute_reply.started": "2024-11-26T12:11:16.151328Z",
          "shell.execute_reply": "2024-11-26T12:11:16.180874Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Poutcome"
      ],
      "metadata": {
        "id": "ULcDUyCgaTtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of target across poutcome categories\n",
        "# Plot the distribution of target with respect to poutcome categories\n",
        "sns.countplot(data=data, x='poutcome', hue='target')\n",
        "plt.title(\"Distribution of 'target' across 'poutcome' categories\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:16.183671Z",
          "iopub.execute_input": "2024-11-26T12:11:16.184118Z",
          "iopub.status.idle": "2024-11-26T12:11:16.481915Z",
          "shell.execute_reply.started": "2024-11-26T12:11:16.184072Z",
          "shell.execute_reply": "2024-11-26T12:11:16.480839Z"
        },
        "id": "BXMOpfGqaTtN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate missing percentage for 'poutcome'\n",
        "poutcome_missing_percentage = data['poutcome'].isnull().mean() * 100\n",
        "print(f\"'poutcome' missing percentage: {poutcome_missing_percentage:.2f}%\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:16.48347Z",
          "iopub.execute_input": "2024-11-26T12:11:16.483928Z",
          "iopub.status.idle": "2024-11-26T12:11:16.491918Z",
          "shell.execute_reply.started": "2024-11-26T12:11:16.483883Z",
          "shell.execute_reply": "2024-11-26T12:11:16.490831Z"
        },
        "id": "gJIOJu8qaTtN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop 'poutcome' if it has limited value for prediction\n",
        "data.drop(['poutcome'], axis=1, inplace=True)\n",
        "test.drop(['poutcome'], axis=1, inplace=True)\n",
        "print(\"Dropped 'poutcome' from both train and test datasets.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:16.49316Z",
          "iopub.execute_input": "2024-11-26T12:11:16.493561Z",
          "iopub.status.idle": "2024-11-26T12:11:16.51267Z",
          "shell.execute_reply.started": "2024-11-26T12:11:16.493505Z",
          "shell.execute_reply": "2024-11-26T12:11:16.511526Z"
        },
        "id": "OoX5S6nLaTtN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling missing values"
      ],
      "metadata": {
        "id": "oLcaroxUaTtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1 Check for null values\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# 2. Handling missing values\n",
        "\n",
        "# For numerical columns, you can fill missing values with mean, median, or any custom value\n",
        "data['age'].fillna(data['age'].mean(), inplace=True)  # Fill with mean\n",
        "data['balance'].fillna(data['balance'].median(), inplace=True)  # Fill with median\n",
        "\n",
        "# For categorical columns, you can fill with the mode or a custom value like 'Unknown'\n",
        "data['job'].fillna(data['job'].mode()[0], inplace=True)  # Fill with mode\n",
        "data['education'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# For the 'contact' column, you can either fill it with 'Unknown' or drop it if it's not useful\n",
        "data['contact'].fillna('Unknown', inplace=True)\n",
        "\n",
        "print('--------------------------------------------------------------------------')\n",
        "\n",
        "# For numerical columns in the test data, fill missing values with the mean, median, or custom value\n",
        "test['age'].fillna(test['age'].mean(), inplace=True)  # Fill with mean\n",
        "test['balance'].fillna(test['balance'].median(), inplace=True)  # Fill with median\n",
        "\n",
        "# For categorical columns, fill with the mode or a custom value like 'Unknown'\n",
        "test['job'].fillna(test['job'].mode()[0], inplace=True)  # Fill with mode\n",
        "test['education'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# For the 'contact' column, fill it with 'Unknown' or drop it if it's not important\n",
        "test['contact'].fillna('Unknown', inplace=True)\n",
        "print(test.isnull().sum())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cWH6mAlUfnCO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:16.514127Z",
          "iopub.execute_input": "2024-11-26T12:11:16.514754Z",
          "iopub.status.idle": "2024-11-26T12:11:16.563305Z",
          "shell.execute_reply.started": "2024-11-26T12:11:16.514716Z",
          "shell.execute_reply": "2024-11-26T12:11:16.562092Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Insights:\n",
        "\n",
        "**1. Checking for Null Values:**\n",
        "\n",
        "* **Purpose:** Identifies columns with missing values.\n",
        "* **Method:** The `isnull().sum()` method calculates the number of null values in each column.\n",
        "\n",
        "**2. Handling Missing Values:**\n",
        "\n",
        "* **Numerical Columns:**\n",
        "   - **Mean Imputation:** Replaces missing values with the mean of the column. This is suitable when the distribution is roughly symmetric.\n",
        "   - **Median Imputation:** Replaces missing values with the median of the column. This is more robust to outliers.\n",
        "* **Categorical Columns:**\n",
        "   - **Mode Imputation:** Replaces missing values with the most frequent category.\n",
        "   - **Custom Value:** Assigns a specific value like 'Unknown' to indicate vohe quality of your analysis.\n"
      ],
      "metadata": {
        "id": "pNSnkTDfaTtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ( ii ) . Handle Categorical Variable :-"
      ],
      "metadata": {
        "id": "7793198a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print unique values of All Categorical data by using for loop.\n",
        "for i in data.keys() :\n",
        "    print(\"★\"*5,i,\"★\"*5,\"\\n\")\n",
        "    print(data[i].value_counts(),'\\n')"
      ],
      "metadata": {
        "id": "261af4ab",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:16.564704Z",
          "iopub.execute_input": "2024-11-26T12:11:16.56505Z",
          "iopub.status.idle": "2024-11-26T12:11:16.609465Z",
          "shell.execute_reply.started": "2024-11-26T12:11:16.565019Z",
          "shell.execute_reply": "2024-11-26T12:11:16.6084Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Insights:\n",
        "1. Displays unique values and counts for each categorical feature.  \n",
        "2. Helps identify imbalances or rare categories.  \n",
        "3. Useful for preprocessing and feature engineering.  "
      ],
      "metadata": {
        "id": "Vk_47RiqaTtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "tut82B_qiE8x",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:16.610567Z",
          "iopub.execute_input": "2024-11-26T12:11:16.610846Z",
          "iopub.status.idle": "2024-11-26T12:11:16.637515Z",
          "shell.execute_reply.started": "2024-11-26T12:11:16.610819Z",
          "shell.execute_reply": "2024-11-26T12:11:16.636246Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Splite Data into Train & Test"
      ],
      "metadata": {
        "id": "KWteNxbjaTtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Split the data into features and target\n",
        "X = data.drop(columns=['target'])  # Features\n",
        "y = data['target']  # Target"
      ],
      "metadata": {
        "id": "MWiVewKtjGUM",
        "trusted": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:16.638567Z",
          "iopub.execute_input": "2024-11-26T12:11:16.638896Z",
          "iopub.status.idle": "2024-11-26T12:11:16.651115Z",
          "shell.execute_reply.started": "2024-11-26T12:11:16.638865Z",
          "shell.execute_reply": "2024-11-26T12:11:16.650058Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Column Transform with scaling and one-hot encoding"
      ],
      "metadata": {
        "id": "tlAJZaLHaTtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Define numerical and categorical columns\n",
        "numerical_cols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous', 'p_year', 'p_month', 'p_weekday']\n",
        "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact']\n",
        "\n",
        "# 4. Create a ColumnTransformer with scaling and one-hot encoding\n",
        "Transformer = ColumnTransformer([\n",
        "    ('scale', StandardScaler(), numerical_cols),\n",
        "    ('onehot', OneHotEncoder(), categorical_cols)\n",
        "], remainder='passthrough')\n",
        "\n",
        "# 5. Fit the transformer on X_train and transform both training and validation data\n",
        "X_train_transformed = pd.DataFrame(Transformer.fit_transform(X_train), columns=Transformer.get_feature_names_out())\n",
        "X_val_transformed = pd.DataFrame(Transformer.transform(X_val), columns=Transformer.get_feature_names_out())\n",
        "test_data_transformed = pd.DataFrame(Transformer.transform(test), columns=Transformer.get_feature_names_out())\n",
        "\n",
        "# 6. Verify the transformed data\n",
        "print(X_train_transformed.head())\n",
        "print(X_val_transformed.head())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:16.652583Z",
          "iopub.execute_input": "2024-11-26T12:11:16.653022Z",
          "iopub.status.idle": "2024-11-26T12:11:16.848499Z",
          "shell.execute_reply.started": "2024-11-26T12:11:16.652975Z",
          "shell.execute_reply": "2024-11-26T12:11:16.847347Z"
        },
        "id": "RF2Pq9blaTtP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Insights :\n",
        "\n",
        "\n",
        "1. **Split Data**: Separates features (`X`) and target (`y`) for training.\n",
        "\n",
        "2. **Train-Validation Split**: Divides data into 80% training and 20% validation for model evaluation.\n",
        "\n",
        "3. **Define Columns**:\n",
        "   - `numerical_cols`: Continuous values for scaling.\n",
        "   - `categorical_cols`: Categorical values for encoding.\n",
        "\n",
        "4. **ColumnTransformer Setup**:\n",
        "   - **StandardScaler** for numerical columns.\n",
        "   - **OneHotEncoder** for categorical columns.\n",
        "   - Keeps other columns unchanged (`remainder='passthrough'`).\n",
        "\n",
        "5. **Transform Data**: Applies transformations to training, validation, and test data for consistent feature scaling and encoding.\n",
        "\n",
        "6. **Data Verification**: Prints transformed training and validation data to confirm trfor model training."
      ],
      "metadata": {
        "id": "24PElCX9aTtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Converting the target variable 'target' into numeric values\n",
        "data['target'] = data['target'].map({'yes': 1, 'no': 0})  #  'yes' and 'no' are the values\n",
        "\n",
        "# 2. Spliting the data into features and target\n",
        "X = data.drop(columns=['target'])  # Features\n",
        "y = data['target']  # Target\n",
        "\n",
        "# 3. Spliting into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "uGUHkk6Uknlx",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:16.855653Z",
          "iopub.execute_input": "2024-11-26T12:11:16.856558Z",
          "iopub.status.idle": "2024-11-26T12:11:16.890743Z",
          "shell.execute_reply.started": "2024-11-26T12:11:16.856522Z",
          "shell.execute_reply": "2024-11-26T12:11:16.889206Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#  Verifingthe transformed data and check for any remaining non-numeric values\n",
        "print(X_train_transformed.dtypes),\n",
        "print(\"******************************\")\n",
        "print(X_val_transformed.dtypes)\n",
        "print(\"******************************\")\n",
        "\n",
        "print(X_train_transformed.dtypes)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:16.892568Z",
          "iopub.execute_input": "2024-11-26T12:11:16.893159Z",
          "iopub.status.idle": "2024-11-26T12:11:16.907874Z",
          "shell.execute_reply.started": "2024-11-26T12:11:16.8931Z",
          "shell.execute_reply": "2024-11-26T12:11:16.906703Z"
        },
        "id": "yKGdH9yUaTtQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "o4zVM03DaTtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline Traning"
      ],
      "metadata": {
        "id": "FNjilE9yaTtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define numerical and categorical columns\n",
        "numerical_cols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous', 'p_year', 'p_month', 'p_weekday']\n",
        "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact']\n",
        "\n",
        "# Create ColumnTransformer for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('scale', StandardScaler(), numerical_cols),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the models you want to evaluate\n",
        "models = [\n",
        "    ('LogisticRegression', LogisticRegression(random_state=42)),\n",
        "    ('RandomForest', RandomForestClassifier(random_state=42)),\n",
        "    ('KNN', KNeighborsClassifier()),\n",
        "    ('XGBClassifier', XGBClassifier(random_state=42)),\n",
        "    ('LGBMClassifier', LGBMClassifier(random_state=42,n_jobs=-1))\n",
        "]\n",
        "\n",
        "# Create a function to train and evaluate models\n",
        "def train_and_evaluate(models, X_train, y_train, X_val, y_val):\n",
        "    best_models = {}\n",
        "\n",
        "    for model_name, model in models:\n",
        "        # Set up the pipeline for each model\n",
        "        pipeline = Pipeline(steps=[\n",
        "            ('preprocessing', preprocessor),\n",
        "            ('pca', PCA(n_components=5)),         # You can adjust n_components later\n",
        "            ('svd', TruncatedSVD(n_components=5)), # Adjust as necessary\n",
        "            ('classifier', model)                 # The current model in the loop\n",
        "        ])\n",
        "\n",
        "        # Train the model\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Predict on the validation set\n",
        "        y_val_pred = pipeline.predict(X_val)\n",
        "\n",
        "        # Calculate F1 score (macro)\n",
        "        f1 = f1_score(y_val, y_val_pred, average='macro')\n",
        "        print(f\"Validation F1 Score (Macro) for {model_name}: {f1:.4f}\")\n",
        "\n",
        "        # Save the best model (optional)\n",
        "        best_models[model_name] = pipeline\n",
        "\n",
        "    return best_models\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate models\n",
        "best_models = train_and_evaluate(models, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Optionally: Predict on test data with the best models\n",
        "for model_name, model in best_models.items():\n",
        "    test_predictions = model.predict(test)\n",
        "    test_predictions_df = pd.DataFrame(test_predictions, columns=['target'])\n",
        "    print(f\"Test Predictions for {model_name}:\")\n",
        "    print(test_predictions_df.head())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:16.909198Z",
          "iopub.execute_input": "2024-11-26T12:11:16.909537Z",
          "iopub.status.idle": "2024-11-26T12:11:33.048606Z",
          "shell.execute_reply.started": "2024-11-26T12:11:16.909506Z",
          "shell.execute_reply": "2024-11-26T12:11:33.045858Z"
        },
        "id": "A1i52xm5aTtR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights:\n",
        "\n",
        "1. **Preprocessing and Dimensionality Reduction**:  \n",
        "   - The `ColumnTransformer` applies scaling and one-hot encoding for numerical and categorical features, respectively.  \n",
        "   - PCA and Truncated SVD reduce the feature space to 5 components, optimizing computational efficiency.\n",
        "\n",
        "2. **Model Performance**:  \n",
        "   - Multiple classifiers are evaluated, with `XGBClassifier` achieving the best F1 Macro score on the validation set (0.6095).  \n",
        "   - `KNN` and `LGBMClassifier` also show competitive performance, highlighting model variation.\n",
        "\n",
        "3. **Predictions and Output**:  \n",
        "   - Test predictions for all models reveal most outputs as class `0`, likely reflecting data imbalances.  \n",
        "   - Model performance could benefit from advanced techniques like oversampling/undersampling or threshold tuning for class balance."
      ],
      "metadata": {
        "id": "PvduUBH3aTtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HyperParameter tuning"
      ],
      "metadata": {
        "id": "TNMe-nb3aTtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "sTzXjQxsaTtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(random_state=42)\n",
        "\n",
        "# Define the parameter distribution for Logistic Regression\n",
        "param_dist_logreg = {\n",
        "    'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],    # Regularization type\n",
        "    'classifier__C': uniform(0.01, 10),                           # Inverse of regularization strength\n",
        "    'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],  # Solver algorithm\n",
        "    'classifier__max_iter': [100, 200, 300, 400, 500]             # Maximum number of iterations\n",
        "}\n",
        "\n",
        "# Create a pipeline with the preprocessor and Logistic Regression\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('classifier', logreg)\n",
        "])\n",
        "\n",
        "# Use RandomizedSearchCV for hyperparameter tuning\n",
        "random_search_logreg = RandomizedSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_distributions=param_dist_logreg,\n",
        "    n_iter=30,                   # Number of parameter settings sampled\n",
        "    cv=5,                        # 5-fold cross-validation for better robustness\n",
        "    scoring='f1_macro',          # Metric for evaluation\n",
        "    n_jobs=-1,                   # Use all available cores\n",
        "    random_state=42              # Ensures reproducibility\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV on the training data\n",
        "random_search_logreg.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and print the best parameters\n",
        "best_model_logreg = random_search_logreg.best_estimator_\n",
        "print(\"Best Parameters for Logistic Regression:\", random_search_logreg.best_params_)\n",
        "\n",
        "# Predict probabilities on validation set\n",
        "y_val_prob_logreg = best_model_logreg.predict_proba(X_val)[:, 1]  # Probabilities for the positive class\n",
        "\n",
        "# Fine-tune threshold to maximize F1 Macro Score\n",
        "thresholds_logreg = np.arange(0.3, 0.6, 0.01)\n",
        "best_f1_macro = 0\n",
        "best_threshold = 0.5  # Default starting threshold\n",
        "\n",
        "for threshold in thresholds_logreg:\n",
        "    y_val_pred_threshold_logreg = (y_val_prob_logreg >= threshold).astype(int)\n",
        "    f1_macro_logreg_threshold = f1_score(y_val, y_val_pred_threshold_logreg, average='macro')\n",
        "    print(f\"Threshold: {threshold:.2f}, F1 Macro Score: {f1_macro_logreg_threshold:.4f}\")\n",
        "\n",
        "    # Update the best F1 macro score and best threshold if a better score is found\n",
        "    if f1_macro_logreg_threshold > best_f1_macro:\n",
        "        best_f1_macro = f1_macro_logreg_threshold\n",
        "        best_threshold = threshold\n",
        "\n",
        "# Print the best F1 macro score and threshold found through tuning\n",
        "print(f\"\\nBest F1 Macro Score after Threshold Tuning: {best_f1_macro:.4f} at Threshold: {best_threshold:.2f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:11:33.050259Z",
          "iopub.execute_input": "2024-11-26T12:11:33.050889Z",
          "iopub.status.idle": "2024-11-26T12:12:50.957807Z",
          "shell.execute_reply.started": "2024-11-26T12:11:33.050824Z",
          "shell.execute_reply": "2024-11-26T12:12:50.956682Z"
        },
        "id": "I8ucl6dLaTtS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights:\n",
        "\n",
        "1. **Optimal Parameters for Logistic Regression**:  \n",
        "   - The model achieved its best configuration with specific hyperparameters: regularization strength (`C`) of 3.35, `l2` penalty, and `newton-cg` solver, ensuring a robust performance.\n",
        "\n",
        "2. **Threshold Tuning**:  \n",
        "   - F1 Macro Score peaks at 0.6436 when the threshold is set to 0.30. As the threshold increases, the F1 score declines, indicating the model's sensitivity to lower thresholds for class balance.\n",
        "\n",
        "3. **Significance of Results**:  \n",
        "   - The optimal threshold (0.30) significantly boosts the F1 score, highlighting the importance of tuning thresholds in imbalanced datasets for achieving better class representation."
      ],
      "metadata": {
        "id": "sRTdPaqZaTtT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RandomForeset Classifier"
      ],
      "metadata": {
        "id": "z80bkMc6aTtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize RandomForest model\n",
        "model_rf = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Parameter distribution for RandomForest (reduced ranges)\n",
        "param_dist_rf = {\n",
        "    'classifier__n_estimators': randint(100, 500),      #  max number of trees\n",
        "    'classifier__max_depth': randint(5, 15),            #  max depth\n",
        "    'classifier__min_samples_split': randint(2, 10),    #  min samples to split a node\n",
        "    'classifier__min_samples_leaf': randint(1, 10),     #  min samples in a leaf node\n",
        "    'classifier__max_features': ['auto', 'sqrt'],       #  number of features to consider\n",
        "    'classifier__bootstrap': [True],                    # Bootstrap sampling\n",
        "    'classifier__class_weight': ['balanced']            # Class weighting\n",
        "}\n",
        "\n",
        "# Create a pipeline with the preprocessor and RandomForest classifier\n",
        "pipeline_rf = Pipeline(steps=[\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('classifier', model_rf)\n",
        "])\n",
        "\n",
        "# RandomizedSearchCV for hyperparameter tuning\n",
        "random_search_rf = RandomizedSearchCV(\n",
        "    estimator=pipeline_rf,\n",
        "    param_distributions=param_dist_rf,\n",
        "    n_iter=15,                  # Reduced number of parameter settings sampled\n",
        "    cv=2,                       # 2-fold cross-validation\n",
        "    scoring='f1_macro',         # Metric for evaluation\n",
        "    n_jobs=-1,                  # Use all available cores\n",
        "    random_state=42             # Ensures reproducibility\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV on the training data\n",
        "random_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and print the best parameters\n",
        "best_model_rf = random_search_rf.best_estimator_\n",
        "print(\"Best Parameters for RandomForest:\", random_search_rf.best_params_)\n",
        "\n",
        "# Predict on validation set and evaluate\n",
        "y_val_pred_rf = best_model_rf.predict(X_val)\n",
        "print(\"\\nRandomForestClassifier Classification Report:\")\n",
        "print(classification_report(y_val, y_val_pred_rf))\n",
        "\n",
        "# Calculate F1 macro score\n",
        "f1_macro_rf = f1_score(y_val, y_val_pred_rf, average='macro')\n",
        "print(f\"F1 Macro Score on Validation Set: {f1_macro_rf:.4f}\")\n",
        "\n",
        "# Threshold Tuning for RandomForest with finer steps\n",
        "y_proba_rf = best_model_rf.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Initialize best F1 macro score and best threshold\n",
        "best_f1_macro = f1_macro_rf  # Start with the initial F1 score\n",
        "best_threshold = 0.5         # Default threshold\n",
        "\n",
        "# Loop through a range of thresholds to find the best one\n",
        "thresholds_rf = np.arange(0.3, 0.6, 0.01)  # Finer steps for threshold\n",
        "for threshold in thresholds_rf:\n",
        "    y_pred_threshold_rf = (y_proba_rf >= threshold).astype(int)\n",
        "    f1_macro_rf_threshold = f1_score(y_val, y_pred_threshold_rf, average='macro')\n",
        "    print(f\"Threshold: {threshold:.2f}, F1 Macro Score: {f1_macro_rf_threshold:.4f}\")\n",
        "\n",
        "    # Update best F1 macro score and best threshold if a better score is found\n",
        "    if f1_macro_rf_threshold > best_f1_macro:\n",
        "        best_f1_macro = f1_macro_rf_threshold\n",
        "        best_threshold = threshold\n",
        "\n",
        "# Print the best F1 macro score and threshold\n",
        "print(f\"Best F1 Macro Score after Threshold Tuning: {best_f1_macro:.4f} at Threshold: {best_threshold:.2f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:12:50.959231Z",
          "iopub.execute_input": "2024-11-26T12:12:50.959575Z",
          "iopub.status.idle": "2024-11-26T12:15:02.229183Z",
          "shell.execute_reply.started": "2024-11-26T12:12:50.959543Z",
          "shell.execute_reply": "2024-11-26T12:15:02.228093Z"
        },
        "id": "qW5eTDBCaTtT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights:\n",
        "\n",
        "1. **Optimal Random Forest Hyperparameters**:  \n",
        "   - The model performs best with a depth of 14, balanced class weights, 154 estimators, and other tuned hyperparameters (`sqrt` for `max_features`, `min_samples_split` of 5, and `min_samples_leaf` of 3). These ensure a well-generalized and balanced approach for the dataset.\n",
        "\n",
        "2. **Classification Report**:  \n",
        "   - Class 0 (majority class) achieves high precision (96%) but slightly lower recall (85%), indicating the model is conservative with predictions.  \n",
        "   - Class 1 (minority class) shows moderate recall (79%) but lower precision (49%), reflecting a need for further optimization on imbalanced data.\n",
        "\n",
        "3. **Threshold Tuning Insights**:  \n",
        "   - The F1 Macro Score peaks at **0.7619** for a threshold of **0.55**, showing that a slightly higher threshold improves the balance between precision and recall for both classes."
      ],
      "metadata": {
        "id": "UZlqHWGIaTtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## K-Nearest Neighbors (KNN):"
      ],
      "metadata": {
        "id": "unR12fgDaTtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# parameter grid for KNN\n",
        "param_grid_knn = {\n",
        "    'classifier__n_neighbors': [5, 7, 10, 15],\n",
        "    'classifier__weights': ['uniform', 'distance'],\n",
        "    'classifier__metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# KNN pipeline without dimensionality reduction\n",
        "pipeline_knn = Pipeline(steps=[\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('classifier', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# GridSearchCV with the pipeline\n",
        "grid_search_knn = GridSearchCV(\n",
        "    pipeline_knn, param_grid_knn, cv=5, scoring='f1_macro', n_jobs=-1\n",
        ")\n",
        "grid_search_knn.fit(X_train, y_train)\n",
        "\n",
        "# Best model evaluation\n",
        "best_knn = grid_search_knn.best_estimator_\n",
        "y_val_pred_knn = best_knn.predict(X_val)\n",
        "print(\"\\nKNN Classification Report:\")\n",
        "print(classification_report(y_val, y_val_pred_knn))\n",
        "\n",
        "# Calculate F1 macro score\n",
        "f1_knn = f1_score(y_val, y_val_pred_knn, average='macro')\n",
        "print(f\"Best KNN F1 Score (Macro) on Validation Set: {f1_knn:.4f}\")\n",
        "print(\"Best Parameters for KNN:\", grid_search_knn.best_params_)\n",
        "\n",
        "# Threshold Tuning for KNN\n",
        "y_proba_knn = best_knn.predict_proba(X_val)[:, 1]  # Get probability estimates for the positive class\n",
        "\n",
        "# Initialize best F1 macro score and best threshold\n",
        "best_f1_macro = f1_knn  # Start with the initial F1 score from default threshold\n",
        "best_threshold = 0.5    # Default threshold\n",
        "\n",
        "# Loop through a finer range of thresholds to find the best one\n",
        "thresholds_knn = np.arange(0.3, 0.6, 0.01)  # Finer steps for threshold tuning\n",
        "for threshold in thresholds_knn:\n",
        "    y_pred_threshold_knn = (y_proba_knn >= threshold).astype(int)\n",
        "    f1_macro_knn_threshold = f1_score(y_val, y_pred_threshold_knn, average='macro')\n",
        "    print(f\"Threshold: {threshold:.2f}, F1 Macro Score: {f1_macro_knn_threshold:.4f}\")\n",
        "\n",
        "    # Update best F1 macro score and best threshold if a better score is found\n",
        "    if f1_macro_knn_threshold > best_f1_macro:\n",
        "        best_f1_macro = f1_macro_knn_threshold\n",
        "        best_threshold = threshold\n",
        "\n",
        "# Print the best F1 macro score and threshold\n",
        "print(f\"Best F1 Macro Score after Threshold Tuning: {best_f1_macro:.4f} at Threshold: {best_threshold:.2f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:15:02.230763Z",
          "iopub.execute_input": "2024-11-26T12:15:02.231206Z",
          "iopub.status.idle": "2024-11-26T12:21:16.204341Z",
          "shell.execute_reply.started": "2024-11-26T12:15:02.23116Z",
          "shell.execute_reply": "2024-11-26T12:21:16.203191Z"
        },
        "id": "3AYuwkM7aTtU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights:\n",
        "\n",
        "1. **KNN Model Performance**:  \n",
        "   - Class 0 (majority class) achieves a high precision (88%) and recall (95%), indicating reliable predictions for the dominant class.  \n",
        "   - Class 1 (minority class) suffers from low recall (27%) and moderate precision (50%), highlighting challenges with imbalanced data.\n",
        "\n",
        "2. **Optimal Parameters and Validation Score**:  \n",
        "   - The best parameters for KNN are `euclidean` distance, `5` neighbors, and `distance` weighting. These settings achieve a validation F1 Macro Score of **0.6312**, showing moderate overall performance.\n",
        "\n",
        "3. **Threshold Tuning Improvement**:  \n",
        "   - Adjusting the threshold improves the F1 Macro Score, peaking at **0.6816** for a threshold of **0.34**, demonstrating that fine-tuning thresholds significantly benefits the balance between precision and recall."
      ],
      "metadata": {
        "id": "o-vSWMHyaTtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "uTjOTOwxaTtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize XGBoost model\n",
        "model_xgb = XGBClassifier(random_state=0, use_label_encoder=False, scale_pos_weight=2)\n",
        "\n",
        "# Expanded parameter distribution for XGBoost\n",
        "param_dist_xgb = {\n",
        "    'classifier__learning_rate': uniform(0.01, 0.3),      # Expanded learning rate range\n",
        "    'classifier__max_depth': randint(3, 15),              # Increased max depth for deeper trees\n",
        "    'classifier__n_estimators': randint(100, 1000),       # Increased n_estimators upper limit\n",
        "    'classifier__subsample': uniform(0.5, 0.9),           # Subsample range\n",
        "    'classifier__colsample_bytree': uniform(0.5, 0.9),    # Feature fraction range\n",
        "    'classifier__gamma': uniform(0, 0.5),                 # Gamma range\n",
        "    'classifier__reg_alpha': uniform(0, 1),               # L1 regularization\n",
        "    'classifier__reg_lambda': uniform(0.5, 1.5)           # L2 regularization\n",
        "}\n",
        "\n",
        "# Create a pipeline with the preprocessor and XGBoost classifier\n",
        "pipeline_xgb = Pipeline(steps=[\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('classifier', model_xgb)\n",
        "])\n",
        "\n",
        "# RandomizedSearchCV for hyperparameter tuning\n",
        "random_search_xgb = RandomizedSearchCV(\n",
        "    estimator=pipeline_xgb,\n",
        "    param_distributions=param_dist_xgb,\n",
        "    n_iter=50,                   # Increased number of parameter settings sampled\n",
        "    cv=5,                        # Increased cross-validation folds\n",
        "    scoring='f1_macro',          # Metric for evaluation\n",
        "    n_jobs=-1,                   # Use all available cores\n",
        "    random_state=42              # Ensures reproducibility\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV on the training data\n",
        "random_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and print the best parameters\n",
        "best_model_xgb = random_search_xgb.best_estimator_\n",
        "print(\"Best Parameters for XGBoost:\", random_search_xgb.best_params_)\n",
        "\n",
        "# Predict on validation set and evaluate\n",
        "y_val_pred_xgb = best_model_xgb.predict(X_val)\n",
        "print(\"\\nXGBoostClassifier Classification Report:\")\n",
        "print(classification_report(y_val, y_val_pred_xgb))\n",
        "\n",
        "# Calculate F1 macro score\n",
        "f1_macro_xgb = f1_score(y_val, y_val_pred_xgb, average='macro')\n",
        "print(f\"F1 Macro Score on Validation Set: {f1_macro_xgb:.4f}\")\n",
        "\n",
        "# Threshold Tuning with finer steps for XGBoost\n",
        "y_proba_xgb = best_model_xgb.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Initialize best F1 macro score and best threshold\n",
        "best_f1_macro = f1_macro_xgb  # Starting with the current F1 score\n",
        "best_threshold = 0.5          # Initial threshold\n",
        "\n",
        "# Loop through a finer range of thresholds to find the best one\n",
        "thresholds_xgb = np.arange(0.3, 0.6, 0.01)  # More granular threshold steps\n",
        "for threshold in thresholds_xgb:\n",
        "    y_pred_threshold_xgb = (y_proba_xgb >= threshold).astype(int)\n",
        "    f1_macro_xgb_threshold = f1_score(y_val, y_pred_threshold_xgb, average='macro')\n",
        "    print(f\"Threshold: {threshold:.2f}, F1 Macro Score: {f1_macro_xgb_threshold:.4f}\")\n",
        "\n",
        "    # Update best F1 macro score and best threshold if a better score is found\n",
        "    if f1_macro_xgb_threshold > best_f1_macro:\n",
        "        best_f1_macro = f1_macro_xgb_threshold\n",
        "        best_threshold = threshold\n",
        "\n",
        "# Print the best F1 macro score and threshold\n",
        "print(f\"Best F1 Macro Score after Threshold Tuning: {best_f1_macro:.4f} at Threshold: {best_threshold:.2f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:21:16.205849Z",
          "iopub.execute_input": "2024-11-26T12:21:16.206265Z",
          "iopub.status.idle": "2024-11-26T12:26:19.236084Z",
          "shell.execute_reply.started": "2024-11-26T12:21:16.20622Z",
          "shell.execute_reply": "2024-11-26T12:26:19.234764Z"
        },
        "id": "-AYpN0bvaTtV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights:\n",
        "\n",
        "1. **XGBoost Model Performance**:  \n",
        "   - **Class 0** (majority class) performs exceptionally well with a high precision of **94%** and recall of **89%**, ensuring accurate identification of non-subscribers.  \n",
        "   - **Class 1** (minority class) shows moderate performance with a precision of **54%** and a recall of **71%**, indicating a focus on capturing subscribers at the cost of some false positives.  \n",
        "   - The overall validation F1 Macro Score is **0.7664**, reflecting a well-balanced model for an imbalanced dataset.\n",
        "\n",
        "2. **Optimal Hyperparameters**:  \n",
        "   - Key parameters such as `learning_rate=0.025`, `max_depth=5`, `n_estimators=570`, and regularization settings (`reg_alpha` and `reg_lambda`) demonstrate careful tuning for effective learning and preventing overfitting.\n",
        "\n",
        "3. **Threshold Tuning Improvement**:  \n",
        "   - Fine-tuning the threshold leads to an optimal F1 Macro Score of **0.7723** at a threshold of **0.40**. This demonstrates the impact of threshold adjustments in improving the balance between precision and recall for both classes.\n",
        "\n",
        "4. **Model Strengths and Application**:  \n",
        "   - XGBoost effectively handles imbalanced datasets, capturing meaningful patterns and achieving the highest F1 Macro Score among all models evaluated.  \n",
        "   - The ability to tune thresholds allows for flexible optimization depending on the business requirement, e.g., prioritizing fewer false negatives or false positives."
      ],
      "metadata": {
        "id": "J_TElhdCaTtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LightGBM"
      ],
      "metadata": {
        "id": "9UXTYP42aTtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define preprocessing for numeric and categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('scaler', StandardScaler(), numerical_cols),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Define the full pipeline with preprocessing, PCA, SVD, and classifier\n",
        "pipeline_lgbm = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LGBMClassifier(random_state=42, verbose=-1))\n",
        "])\n",
        "\n",
        "# Define parameter grid for hyperparameter tuning\n",
        "param_dist_lgbm = {\n",
        "    'classifier__n_estimators': [50, 100, 150],\n",
        "    'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
        "    'classifier__max_depth': [5, 10, 20],\n",
        "    'classifier__num_leaves': [31, 50, 100],\n",
        "    'classifier__min_data_in_leaf': [10, 20, 30]\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "random_search_lgbm = RandomizedSearchCV(\n",
        "    pipeline_lgbm, param_distributions=param_dist_lgbm, n_iter=10, cv=5,\n",
        "    scoring='f1_macro', n_jobs=-1, random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model with training data\n",
        "random_search_lgbm.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from RandomizedSearchCV\n",
        "best_lgbm = random_search_lgbm.best_estimator_\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred_lgbm = best_lgbm.predict(X_val)\n",
        "f1_lgbm = f1_score(y_val, y_val_pred_lgbm, average='macro')\n",
        "print(f\"\\nBest LightGBM F1 Score (Macro): {f1_lgbm:.4f}\")\n",
        "print(\"Best Parameters for LightGBM:\", random_search_lgbm.best_params_)\n",
        "\n",
        "# Print classification report for initial threshold of 0.5\n",
        "print(\"\\nLightGBM Classifier Classification Report (Threshold=0.5):\")\n",
        "print(classification_report(y_val, y_val_pred_lgbm))\n",
        "\n",
        "# Threshold Tuning\n",
        "y_proba_lgbm = best_lgbm.predict_proba(X_val)[:, 1]  # Probability for the positive class\n",
        "\n",
        "# Initialize variables to find the optimal threshold\n",
        "best_f1_macro = f1_lgbm  # Start with initial F1 score\n",
        "best_threshold = 0.5     # Default threshold\n",
        "\n",
        "# Fine-tune threshold to maximize F1 macro score\n",
        "thresholds_lgbm = np.arange(0.3, 0.6, 0.01)\n",
        "for threshold in thresholds_lgbm:\n",
        "    y_pred_threshold_lgbm = (y_proba_lgbm >= threshold).astype(int)\n",
        "    f1_macro_lgbm_threshold = f1_score(y_val, y_pred_threshold_lgbm, average='macro')\n",
        "    print(f\"Threshold: {threshold:.2f}, F1 Macro Score: {f1_macro_lgbm_threshold:.4f}\")\n",
        "\n",
        "    # Update the best F1 macro score and best threshold if a better score is found\n",
        "    if f1_macro_lgbm_threshold > best_f1_macro:\n",
        "        best_f1_macro = f1_macro_lgbm_threshold\n",
        "        best_threshold = threshold\n",
        "\n",
        "# Print the best F1 macro score and threshold found through tuning\n",
        "print(f\"\\nBest F1 Macro Score after Threshold Tuning: {best_f1_macro:.4f} at Threshold: {best_threshold:.2f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T13:16:00.426784Z",
          "iopub.execute_input": "2024-11-26T13:16:00.42721Z",
          "iopub.status.idle": "2024-11-26T13:16:35.860337Z",
          "shell.execute_reply.started": "2024-11-26T13:16:00.427176Z",
          "shell.execute_reply": "2024-11-26T13:16:35.859065Z"
        },
        "id": "Gk8RnzeVaTtV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights:\n",
        "\n",
        "1. **LightGBM Model Performance (Default Threshold of 0.5)**:\n",
        "   - **Class 0** (non-subscribers) shows strong performance with **89% precision** and **95% recall**, ensuring accurate identification.\n",
        "   - **Class 1** (subscribers) has moderate precision of **57%** and lower recall at **37%**, indicating more missed detections of true subscribers.\n",
        "   - The macro-average F1 Score is **0.6855**, reflecting moderate performance on an imbalanced dataset.\n",
        "\n",
        "2. **Optimal Hyperparameters**:\n",
        "   - The model's parameters include `num_leaves=100`, `n_estimators=100`, `min_data_in_leaf=10`, `max_depth=10`, and a learning rate of **0.1**, which balance complexity and training efficiency.\n",
        "\n",
        "3. **Threshold Tuning Impact**:\n",
        "   - Lowering the threshold to **0.30** improves the F1 Macro Score to **0.7413**, highlighting LightGBM's potential to better balance precision and recall when prioritizing Class 1 (subscribers).\n",
        "   - However, as the threshold increases, the F1 score decreases, indicating diminished recall for Class 1.\n",
        "\n",
        "4. **Comparison to Other Models**:\n",
        "   - LightGBM's best F1 Macro Score after tuning (**0.7413**) is lower than XGBoost (**0.7723**) and Random Forest (**0.7619**) but outperforms KNN (**0.6816**).\n",
        "   - It may still be a viable choice when training time and interpretability aineering techniques."
      ],
      "metadata": {
        "id": "TngOisAnaTtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison of all Models:"
      ],
      "metadata": {
        "id": "CbqO1lQIaTtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Model names and F1 macro scores\n",
        "models = ['Logistic Regression', 'Random Forest', 'KNN', 'XGBoost', 'LightGBM']\n",
        "f1_scores = [0.6436, 0.7619, 0.6816, 0.7723, 0.7334]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(models, f1_scores, color=['blue', 'green', 'orange', 'red', 'purple'])\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Best F1 Macro Score After Threshold Tuning')\n",
        "plt.title('Comparison of F1 Macro Scores for Different Models')\n",
        "\n",
        "# Adding numbers on top of the bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.01, f'{yval:.4f}', ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "plt.ylim(0.6, 0.8)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T12:28:09.936815Z",
          "iopub.execute_input": "2024-11-26T12:28:09.937201Z",
          "iopub.status.idle": "2024-11-26T12:28:10.228957Z",
          "shell.execute_reply.started": "2024-11-26T12:28:09.937168Z",
          "shell.execute_reply": "2024-11-26T12:28:10.227838Z"
        },
        "id": "7pv4fLCgaTtW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bar chart illustrates the **comparison of F1 Macro Scores** achieved by different models after threshold tuning. The models included are:\n",
        "\n",
        "1. **Logistic Regression**: Achieved the lowest score of **0.6436**.\n",
        "2. **Random Forest**: Performed well with an F1 Macro Score of **0.7619**, making it one of the top-performing models.\n",
        "3. **K-Nearest Neighbors (KNN)**: Scored moderately with **0.6816**, better than Logistic Regression but lagging behind tree-based methods.\n",
        "4. **XGBoost**: Achieved the highest score of **0.7723**, indicating its superior performance among the evaluated models.\n",
        "5. **LightGBM**: Scored **0.7334**, slightly behind Random Forest and XGBoost but significantly better than KNN and Logistic Regression.\n",
        "\n",
        "The chart highlights that **XGBoost** is the best-performing model in terms of F1 Macro Score, followed by **Random Forest** and **LightGBM**, while **Logistic Regression** and **KNN** perform less effectively. This analysis emphasizes the dominance of tree-based ensemble methods (XGBoost and Random Forest) for this classification task."
      ],
      "metadata": {
        "id": "eLJOna1RaTtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submission of Best Model"
      ],
      "metadata": {
        "trusted": true,
        "id": "X8Xf_h8NaTtW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Fit the RandomizedSearchCV on the training data\n",
        "random_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Step 2: Get the best model and print the best parameters\n",
        "best_model_xgb = random_search_xgb.best_estimator_\n",
        "print(\"Best Parameters for XGBoost:\", random_search_xgb.best_params_)\n",
        "\n",
        "# Step 3: Predict probabilities on the validation set\n",
        "y_proba_xgb = best_model_xgb.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Step 4: Threshold tuning\n",
        "best_threshold = 0.5\n",
        "best_f1_macro = 0\n",
        "thresholds_xgb = np.arange(0.3, 0.6, 0.01)  # Fine-grained thresholds from 0.3 to 0.6\n",
        "\n",
        "for threshold in thresholds_xgb:\n",
        "    y_pred_threshold_xgb = (y_proba_xgb >= threshold).astype(int)\n",
        "    f1_macro_xgb_threshold = f1_score(y_val, y_pred_threshold_xgb, average='macro')\n",
        "    print(f\"Threshold: {threshold:.2f}, F1 Macro Score: {f1_macro_xgb_threshold:.4f}\")\n",
        "    if f1_macro_xgb_threshold > best_f1_macro:\n",
        "        best_f1_macro = f1_macro_xgb_threshold\n",
        "        best_threshold = threshold\n",
        "\n",
        "print(f\"\\nBest F1 Macro Score after Threshold Tuning: {best_f1_macro:.4f} at Threshold: {best_threshold:.2f}\")\n",
        "\n",
        "# Step 5: Predict probabilities on the test set using the best threshold\n",
        "y_test_proba = best_model_xgb.predict_proba(test)[:, 1]\n",
        "y_test_pred = (y_test_proba >= best_threshold).astype(int)\n",
        "\n",
        "# Step 6: Map predictions to \"yes\" or \"no\"\n",
        "y_pred_mapped = np.where(y_test_pred == 1, 'yes', 'no')\n",
        "\n",
        "# Step 7: Prepare the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': range(len(y_pred_mapped)),  # If 'id' column is unavailable, generate index-based IDs\n",
        "    'target': y_pred_mapped\n",
        "})\n",
        "\n",
        "# Step 8: Save the submission file\n",
        "submission.to_csv('submission.csv', index=False, header=True)\n",
        "\n",
        "# Step 9: Preview the submission file\n",
        "print(\"Submission File Preview:\")\n",
        "print(submission.head())\n",
        "print(submission.shape)\n",
        "print(submission['target'].value_counts())\n",
        "\n",
        "print(\"\\nFinal submission file created successfully with the tuned XGBoost model!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T14:43:53.156863Z",
          "iopub.execute_input": "2024-11-26T14:43:53.157583Z",
          "iopub.status.idle": "2024-11-26T14:43:53.42237Z",
          "shell.execute_reply.started": "2024-11-26T14:43:53.157544Z",
          "shell.execute_reply": "2024-11-26T14:43:53.420991Z"
        },
        "id": "rsesN7tLaTtX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this project, I used machine learning techniques to predict the success of bank telemarketing campaigns. After performing data preprocessing and exploratory data analysis (EDA), I experimented with multiple models including Logistic Regression, Random Forest, KNN, LightGBM, and XGBoost.\n",
        "\n",
        "**Key Points:**\n",
        "- **XGBoost** achieved the best performance with a final **F1 Macro Score of 0.7723** after fine-tuning the decision threshold.\n",
        "- The project demonstrated the importance of feature engineering, model selection, and threshold adjustments to improve model performance.\n",
        "\n",
        "This model can help businesses optimize marketing campaigns, enhancing customer targeting and resource allocation.\n"
      ],
      "metadata": {
        "id": "0Gy8x7QJaTtX"
      }
    }
  ]
}
